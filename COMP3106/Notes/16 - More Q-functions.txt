What is the differece between a value function and a Q-function?
Value functions estimate long term reward for states
Q functions estimate long term reward for state-action pairs

Initially Estimate:

Q(s,a) = {	+1	if s = G+1	
		-1	if s = G-1	
		0.5	otherwise	}

Q((2,0), -> ) <- Q((2,0), -> ) + alpha(r(2,0) + max_a'(Q((2,1), -> ), Q((2,1), <- )) - Q((2,0, -> ))
	<- 0.5 + 0.1(-0.1 + max(0.5,0.5) - 0.5)
	<- 0.49

Q((2,1), -> ) <- 0.5 + 0.1(-0.1 + max(0.5,0.5) - 0.5)
	<- 0.49
Q((2,2), ^ ) <- 0.49
Q((1,2), ^ ) <- 0.49 
Q((0,2), <- ) <- 0.5 + 0.1(-0.1 + max(+1) - 0.5)
	<- 0.54
Q((2,0), ^ ) <- 0.49
Q((1,0), ^ ) <- 0.44

Q((2,0), -> ) <- 0.49 + 0.1(-0.1 + max(0.49, 0.5) -0.5)
	<- 0.481

Q-Learning with Large State Space
We approximate V or Q using function approximation

V(s) = 0_1*f_1(s) + 0_2*f_2(s) + ... + 0_n*f_n(s)
Q(s,a) = 0_1*f_1(s,a) + 0_2*f_2(s,a) + ... + 0_n*f_n(s,a)

Example - Tic Tac Toe agent

f_1(s) = # of X in corners
f_2(s) = # of X in center
f_3(s) = # of rows/columns/diagonals with 2 Xs

_|_|X
_|_|X
o| |


f_1(s) = 1
f_2(s) = 0
f_3(s) = 1

How do we use this?
V(s) <- V(s) + alpha*(error)
Q(s,a) <- Q(s,a) + alpha*(error)

How do we update paramters?
0_i <- 0_i + alpha * error * f_i(s)
0_i <- 0_i + alpha * (r(s) + gamma * V(s') - V(s))* f_i(s)
0_i <- 0_i + alpha * error * f_i(s,a)
0_i <- 0_i + alpha * (r(s) + gamma * max_a' Q(s',a') - Q(s,a))* f_i(s,a)

Example:

G-1	G+1	<-
^	X	^
^->	->	^

r(s) = {-1 if s = G-1
	+1 if s = G+1
	-0.1 otherwise	}

f_1(s,a) = D_man(s, G+1)
Q(s,a) = 0_1f_1(s,a)

0_1 <- 0_1 _ alpha*error*f_1(s,a)
Initially: teeta_1 = 0.5
0_1 <- 0_1 + alpha(r(2,0) + max_a'(0.5 * 2, 0.5 * 2) - 0.5 * 3) * 3
	<- 0.5 + 0.1(-0.1 + max(1,1) - 1.5) * 3
	<- 0.32

0_1 <- 0_1 + alpha(r(2,1) + max_a'(0_1*D_man((2,2),(G+1)) - 0.32 * 2) * 2
	<- 0.364

Q((2,1), <- ) = 0_1 * D_man((2,1),G+1)
	= 0.728

Direct Policy Search
Previously:
	pi(s) = max_a Q(s,a)
Estimate Q as:
	Q(s,a) = 0_1f_1(s,a) + ... + 0_nf_n(s,a)

Plugging this in
	pi(s) = max_a [0_1f_1(s,a) + ... + 0_nf_n(s,a)]

Can we directly tweak 0_1,...,0_n?
Iterate the following:
1. Generate trajectory with policies
	pi(0) and pi(0+delta0)
2. Assess rewards associated with policies
3. Update policy to be 0 + delta0 if the rewards are higher
"Powell's method"
